{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf4iSTPFUs1_"
   },
   "source": [
    "# COMS W4732 Homework 2: Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DU2tIu0Us2C"
   },
   "source": [
    "Following the machine learning content covered in class, in this assignment we will explore some crucial concepts in gradient descent and backpropgation. \n",
    "\n",
    "Specifically, in Section 1, we will work with a **feedforward neural network** (also known as a **multi-layer perceptron**) implemented solely in numpy, reflect on the associated details of the **forward** pass and implement the **backpropagation** parts of our layers to train our MLP. In Section 2, we will independently look at some gradient-based optimization techniques that are popular with convex functions and have been shown to be useful in finding sufficiently satisfactory optima on the loss manifolds on parametrized models. \n",
    "\n",
    "Your job is to implement the sections marked with `TODO` to complete the tasks. Your tasks on this homework will be:\n",
    "\n",
    "## Section 1 (40 points) \n",
    "Review the details on the chain rule and the backproprapagation step from lecture. You should take a look at the guide we provide to get familiar with the forward pass and backward pass equations for a Multi-Layer Perceptron. You are required to implement all parts marked with a **TO DO:**. The goal of this assignment is to leave you with a very through understanding of forward pass as well as the backpropgation mechanics of a Multi-layer Peceptron. Namely, you will be working with:\n",
    "\n",
    "* a **linear** layer with **Leaky ReLU** \n",
    "* a **linear** layer with a custom activation function that has a learnable parameter\n",
    "* a **linear** output layer\n",
    "* a **softmax cross-entropy** Loss layer \n",
    "\n",
    "## Section 2 (60 points). \n",
    "We will introduce different gradient-based iterative optimization techniques which came from the domain of convex optimization and which have since been adapted for loss functions of modern neural networks that have millions of parameters. Try out a few of these to appreciate the improvements they make on each other. Specifically, we will look at:\n",
    "    \n",
    "* **Full Gradient Descent**\n",
    "* **Stochastic Gradient Descent**\n",
    "* **Stochastic Gradient Descent + Momentum**\n",
    "* **AdaGrad**\n",
    "* **Adaptive Moment Estimation (ADAM)**\n",
    "\n",
    "### About Submission\n",
    "\n",
    "- Please submit the notebook (ipynb and pdf) including the output of all cells. Please note that you should export your completed notebook as a PDF (CV2_HW2_UNI.pdf) and upload it to GradeScope.\n",
    "- Then, please submit the executable completed notebook (CV2_HW2_UNI.ipynb) to Cousework.\n",
    "- For both files, 1) remember to replace <UNI> with your own uni; 2) the completed notebook should show your code, as well as the final image you created.\n",
    "\n",
    "### Before your implementation\n",
    "\n",
    "- Please check the packages listed in the **requirements.txt**. You can also use `pip install -r requirements.txt` to install the packages directly.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNBUvCPKUs2D"
   },
   "source": [
    "# Section 1: Backpropagation\n",
    "\n",
    "This assignment is aimed at leaving you with a very solid understanding of how backpropogation works in the context of a Neural Network. We have provided most of the code for a MLP (Multi-layer Perceptron) written completely in Numpy with some functions left for you to implement to get the network up and running. \n",
    "\n",
    "On correct completion, you will be able to successfully train your MLP for any classification problem where the input feature vector is relatively low-dimensional. We've provided code that pre-processes and trains your MLP on the **Red Wine Classification** dataset for binary-classification. This is to enable you to quickly run and test your MLP. \n",
    "\n",
    "## Lets get to it!\n",
    "\n",
    "As the building blocks of our MLP, we define three layer classes: Hidden, Output and Loss. \n",
    "Each of these inherit from our Base class and thus implement self.forward_pass(), self.backward_pass(), and contain self.update_weighhts(), which is already implemented. \n",
    "\n",
    "Before we dive in, we would like to highlight the distinction between the Hidden Layer and the Hidden_Vondrick layer as you will see defined below in the code.\n",
    "\n",
    "### Hidden Layer\n",
    "This is a standard Hidden Layer that uses Leaky ReLU as its activation function. Remember that Leaky ReLU is defined as a piecewise function:\n",
    "\n",
    "$g(x)= \\bigg\\{\n",
    "\\begin{aligned}\n",
    "& x:& x>0,&\\\\\n",
    "& 0.01x:& x<=0 \n",
    "\\end{aligned}$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Hidden_Vondrick Layer (utilizes cuztom activation function with learnable parameter)\n",
    "This layer is similar to the standard Hidden layer, but with one notable exception. We will use a custom activation function g'(x). Furthermore, we will make exponent parameter 'n' learnable, and update it also leveraging the chain rule and backpropgation.\n",
    "\n",
    "$g'(x)= \\bigg\\{\n",
    "\\begin{aligned}\n",
    "& x^n:& x>0,&\\\\\n",
    "& 0.01x:& x<=0 \n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Go over the code for the MLP throughly and understand each update equation implemented as code. The forward_pass() method is implemented for you for every layer and thus you may find printing out variables and their shapes useful, before you begin implementing the backpropogation methods. Having a clear understanding of the forward and backward pass formulae is crucial for this Section. Your job is to implement only sections marked as **TO DO:** (7 in total)\n",
    "(The PDF in the zip file is for your reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "yshYjJWkUs2E"
   },
   "outputs": [],
   "source": [
    "class Base:\n",
    "    def __init__(self, input_dims:int, output_dims:int):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "\n",
    "    def forward_pass(self):\n",
    "        pass \n",
    "    \n",
    "    def backward_pass(self):\n",
    "        pass\n",
    "\n",
    "    def update_weights(self, W, b, del_W, del_b, learning_rate):\n",
    "        W-=(learning_rate*del_W)\n",
    "        b-=(learning_rate*del_b)\n",
    "        return W, b\n",
    "\n",
    "class Hidden(Base):\n",
    "    def __init__(self, input_dims:int, output_dims:int):\n",
    "        super().__init__(input_dims, output_dims)\n",
    "        \n",
    "        self.W = np.random.random((input_dims, output_dims)) - 0.5\n",
    "        self.c = np.random.random((1, output_dims)) - 0.5\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        U =X@ self.W + self.c\n",
    "        activations= self.leaky_relu(U)\n",
    "        return activations\n",
    "\n",
    "    def backward_pass(self, X, h, dLdh, alpha, learning_rate):\n",
    "        \"\"\"\n",
    "        TO DO:  Finish this backward_pass method by completing the lines marked by the #s. \n",
    "        Remember to use the helper functions update_weights(), leaky_relu, and leaky_relu_derivative wherever needed.\n",
    "        \"\"\"\n",
    "\n",
    "        # relu_derivative here is dhdU\n",
    "        relu_derivative = self.leaky_relu_derivative(h)\n",
    "        dLdU = relu_derivative * dLdh\n",
    "        # As alpha is added as a regularization parameter, \n",
    "        # that will change the closed form solution in the loss function \n",
    "        # thus, giving us the 2nd term in the formula below.\n",
    "        dLdW = X.T @ dLdU + alpha * self.W\n",
    "        # Applying the chain rule, we can achieve this formulation.\n",
    "        dLdc = np.mean(dLdU, axis = 0)\n",
    "        for_prev = dLdU @ self.W.T                                                            \n",
    "\n",
    "        self.W, self.c = self.update_weights(self.W, self.c, dLdW, dLdc, learning_rate)\n",
    "        return for_prev \n",
    "        \"\"\"Note: for_prev is the gradient dL/dh' we pass onto the previous layer h' \"\"\" \n",
    "\n",
    "    def leaky_relu(self, inp):\n",
    "        activation_mask = 1.0 * (inp >0) + 0.01*(inp<0)\n",
    "        activations= np.multiply(inp, activation_mask)\n",
    "        return activations \n",
    "\n",
    "    def leaky_relu_derivative(self, h):\n",
    "        \"\"\"\n",
    "        TO DO: Implement the leaky_relu_derivative method. \n",
    "        This should return a numpy ndarray of shape (batch_size x self.output_dims)\n",
    "        \"\"\"\n",
    "        leaky_relu_derivative = np.zeros_like(h)\n",
    "        input_dim, output_dim = h.shape\n",
    "        \n",
    "        for row_idx in range(input_dim):\n",
    "            for col_idx in range(output_dim):\n",
    "                if h[row_idx][col_idx] > 0:\n",
    "                    leaky_relu_derivative[row_idx][col_idx] = 1.0\n",
    "                else:\n",
    "                    leaky_relu_derivative[row_idx][col_idx] = 0.01\n",
    "                    \n",
    "        return leaky_relu_derivative\n",
    "\n",
    "class Hidden_Vondrick(Base):\n",
    "    def __init__(self, input_dims:int, output_dims:int):\n",
    "        super().__init__(input_dims, output_dims)\n",
    "        \n",
    "        self.W = np.random.random((input_dims, output_dims))\n",
    "        self.c = np.random.random((1, output_dims))\n",
    "        self.U= None\n",
    "        self.vondrick_exponent= np.random.uniform(1.4,2) #The learnable exponent, called Vondrick Exponent, for our custom activation function is initilized from a Unifom(1.4,2) distribution. You may change this if you really want to, but keep it close to this range to ensure training stability.\n",
    "\n",
    "        print(\"Intital Value of Vondrick Exponent: \"+ str(self.vondrick_exponent) )\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.U =X@ self.W + self.c\n",
    "        #Applys the custom activation elementwise\n",
    "        activations= self.vondrick_activation(self.U)\n",
    "        return activations\n",
    "\n",
    "    def backward_pass(self, X, h, dLdh, alpha, learning_rate=0.0005):\n",
    "    \n",
    "        \"\"\"\n",
    "        # TO DO: Fill in this  backward pass method.\n",
    "        \"\"\"\n",
    "        \n",
    "        derivative_wrt_U, derivative_wrt_exponent = self.vondrick_activation_derivative()\n",
    "        \n",
    "        # Needed derivatives\n",
    "        dLdU = derivative_wrt_U * dLdh\n",
    "        dLdW = X.T @ dLdU + alpha * self.W\n",
    "        dLdc = np.mean(dLdU, axis = 0)\n",
    "        \n",
    "        # Needed for the previous layer\n",
    "        for_prev = dLdU @ self.W.T\n",
    "        \n",
    "        # dLdn\n",
    "        dL_dexponent_scalar = np.mean(derivative_wrt_exponent * dLdh)\n",
    "        \n",
    "        #Note, that for the purposes of training stablity, we have hard-coded the learning rate here to be 0.0005\n",
    "        self.W, self.c = self.update_weights(self.W, self.c, dLdW, dLdc, 0.0005) \n",
    "\n",
    "        #Gradient Descent on our learnable activation function parameter: Updating exponent, but clipping it's lower range to 1.01\n",
    "        self.vondrick_exponent= max(1.01,self.vondrick_exponent- 0.001*dL_dexponent_scalar )\n",
    "\n",
    "        return for_prev\n",
    "\n",
    "    def vondrick_activation(self, U):\n",
    "        \"\"\"\n",
    "        TO DO:\n",
    "        Implement this helper function that the forward pass uses compute to compute the activation \n",
    "        map for the given input U.\n",
    "\n",
    "        return activations: (batch_size x output_dims)\n",
    "        \"\"\"\n",
    "        \n",
    "        activation_mask = (U**self.vondrick_exponent) * (U > 0) + 0.01 * (U < 0)\n",
    "        activations = np.multiply(U, activation_mask)\n",
    "        \n",
    "        return activations \n",
    "\n",
    "    def vondrick_activation_derivative(self):\n",
    "        \"\"\"\n",
    "        # TO DO:\n",
    "        Implement this helper function that uses the stored self.U to do a backward pass and \n",
    "        return both dh/dU and dh/dexponent. Both should be numpy matrices dimensions batch_size x self.output_dims \n",
    "\n",
    "        return activations: (batch_size x self.output_dims)\n",
    "        \"\"\"\n",
    "        \n",
    "        derivative_wrt_U =  np.zeros_like(self.U)\n",
    "        derivative_wrt_exponent = np.zeros_like(self.U)\n",
    "        num_rows, num_cols = self.U.shape\n",
    "        \n",
    "        for row_idx in range(num_rows):\n",
    "            for col_idx in range(num_cols):\n",
    "                if self.U[row_idx][col_idx] > 0:\n",
    "                    derivative_wrt_U[row_idx][col_idx] = self.vondrick_exponent * \\\n",
    "                                                        (self.U[row_idx][col_idx]**(self.vondrick_exponent - 1))\n",
    "                    derivative_wrt_exponent[row_idx][col_idx] = (self.U[row_idx][col_idx] ** \\\n",
    "                                                                 self.vondrick_exponent) * \\\n",
    "                                                                np.log(self.U[row_idx][col_idx])\n",
    "                else:\n",
    "                    derivative_wrt_U[row_idx][col_idx] = 0.01\n",
    "                    derivative_wrt_exponent[row_idx][col_idx] = 0.0\n",
    "        \n",
    "        return derivative_wrt_U, derivative_wrt_exponent\n",
    "\n",
    "    \n",
    "class Output(Base):\n",
    "\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__(input_dims, output_dims)\n",
    "        self.w = np.random.random((input_dims, output_dims)) -0.5\n",
    "        self.b = np.random.random((1, output_dims)) -0.5\n",
    "\n",
    "    def forward_pass(self, h):\n",
    "        z = h @self.w + self.b\n",
    "        z = z - np.max(z, axis = 1).reshape(z.shape[0], 1) # trick: subtracting maz z as softmax is not effected: prevents overflow when we do exponentation\n",
    "        return z\n",
    "\n",
    "    def backward_pass(self, h, dLdz, alpha, learning_rate):\n",
    "        \"\"\" \n",
    "        # TO DO: Implement the backward pass for the output layer. \n",
    "        Finally, update the Weight matrix and bias vector appropriately, and then return dLdh, which will be passed backed to previous layers during backpropgation\n",
    "        \"\"\"\n",
    "        dLdh = dLdz @ self.w.T\n",
    "        dLdw = h.T @ dLdz + alpha * self.w\n",
    "        dLdb = dLdz[0]\n",
    "        \n",
    "        self.w, self.b = self.update_weights(self.w, self.b, dLdw, dLdb, learning_rate)\n",
    "        return dLdh\n",
    "\n",
    "class Loss(Base): \n",
    "    \n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__(input_dims, output_dims)\n",
    "    \n",
    "    def forward_pass(self, z, y):\n",
    "\n",
    "        temp = -z + np.log(np.sum(np.exp(z), axis = 1)).reshape(z.shape[0], 1) #Computing Softmax Cross Entropy Loss terms for each z_i. Note dimensions of temp: batch_size x output layer output_dims\n",
    "        L = temp[np.arange(z.shape[0]), y.flatten().astype(int)] #Extracts Loss term corresponding only to ground truth class from each row (sample). \n",
    "        L = np.mean(L) #Mean Loss over the batch\n",
    "        return L \n",
    "\n",
    "    def backward_pass(self, z, y):\n",
    "        #Recall the simplified expression we get for dL_i/dz_k= p_k- I(y_i=k) (Details in the guide)\n",
    "        temp1 = np.zeros(z.shape)\n",
    "        for i in range(z.shape[0]):\n",
    "            true_class = int(y[i].item())\n",
    "            temp1[i][true_class] = -1     #-1 is added to the loss term corresponding to the true class\n",
    "\n",
    "        temp2 = np.exp(z)/ np.sum(np.exp(z), axis =1 ).reshape(z.shape[0], 1) #Matrik of p_k terms, aka, elements replaced by softmaxed probabilities\n",
    "        for_previous = temp1 + temp2 \n",
    "        return for_previous\n",
    "    \n",
    "class NN: \n",
    "    def __init__(self): \n",
    "        self.output_layer= self.loss_layer =  None \n",
    "        self.hidden_layers = []\n",
    "\n",
    "    def add_layer(self, name, input_dims, output_dims):\n",
    "        if name.lower() == 'hidden':\n",
    "            self.hidden_layers.append(Hidden(input_dims, output_dims))\n",
    "        elif name.lower() == 'hidden_vondrick':\n",
    "            self.hidden_layers.append(Hidden_Vondrick(input_dims, output_dims))\n",
    "        elif name.lower() =='output':\n",
    "            self.output_layer = Output(input_dims, output_dims)\n",
    "        elif name.lower() =='loss':\n",
    "            self.loss_layer = Loss(input_dims, output_dims) \n",
    "    \n",
    "    def forward_prop(self, X, y, alpha): \n",
    "        hidden_outputs = []\n",
    "        z = L = h= None \n",
    "        for layer in self.hidden_layers:\n",
    "            h = layer.forward_pass(X)\n",
    "            hidden_outputs.append(h)\n",
    "            X = h \n",
    "        z = self.output_layer.forward_pass(h)\n",
    "        L = self.loss_layer.forward_pass(z, y)\n",
    "        for layer in self.hidden_layers:\n",
    "            L += 0.5*alpha*np.linalg.norm(layer.W)**2\n",
    "        L+= 0.5*alpha* np.linalg.norm(self.output_layer.w)**2 \n",
    "        return hidden_outputs, z, L \n",
    "\n",
    "    def backward_prop(self, X, hidden_outputs, z, y, alpha =0.01, learning_rate =0.01):\n",
    "        dLdz = self.loss_layer.backward_pass(z, y)\n",
    "        for_previous = self.output_layer.backward_pass(hidden_outputs[-1], dLdz, alpha, learning_rate) \n",
    "        for i in range(len(self.hidden_layers)-1,0,-1):\n",
    "            temp = self.hidden_layers[i].backward_pass(hidden_outputs[i-1], hidden_outputs[i], for_previous, alpha, learning_rate)\n",
    "            for_previous = temp \n",
    "        self.hidden_layers[0].backward_pass(X, hidden_outputs[0], for_previous, alpha, learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size, learning_rate, alpha, show_training_accuracy=True): \n",
    "  \n",
    "        loss = []\n",
    "        for epoch in range(epochs):\n",
    "            predicted = self.predict(X)\n",
    "            correct = 0 \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct+=1\n",
    "            if show_training_accuracy:\n",
    "                print(f'the accuracy on the training data after epoch {epoch + 1} is {correct/X.shape[0]}')\n",
    "            temp = total = 0 \n",
    "            for k in range(0, X.shape[0], batch_size):\n",
    "                inp = X[k:k+batch_size]\n",
    "                out = y[k:k+batch_size]\n",
    "\n",
    "                hidden_outputs, z, L = self.forward_prop(inp, out, alpha)\n",
    "                temp+=L \n",
    "                total+=1\n",
    "                self.backward_prop(inp, hidden_outputs, z, out, alpha, learning_rate)\n",
    "            \n",
    "            loss.append(temp/total)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X): \n",
    "        \"\"\"\n",
    "        TO DO:\n",
    "        Implement the predict() method that takes in a batch input X \n",
    "        (number_of_samples x feauture_vector_dims) and \n",
    "        returns an nparray y of predictions (number_of_samples x 1)\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_outputs = list()\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            h = layer.forward_pass(X)\n",
    "            hidden_outputs.append(h)\n",
    "            X = h \n",
    "        z = self.output_layer.forward_pass(h)\n",
    "        predictions = [np.argmax(pred) for pred in z]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        predicted_Y= self.predict(X)\n",
    "        correct=0\n",
    "        for i in range(len(predicted_Y)):\n",
    "            if predicted_Y[i] == Y[i]:\n",
    "                correct+=1\n",
    "        return correct/len(Y)\n",
    "\n",
    "def plot_loss(loss_li):\n",
    "    #Given a list of losses over the epochs, plots the loss curve.\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"loss of the neural network per epoch\")\n",
    "    plt.plot(loss_li)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your MLP: Red Wine Quality Classification Dataset\n",
    "\n",
    "## More about the dataset:\n",
    "https://archive.ics.uci.edu/ml/datasets/wine+quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using colab, pls refer to commands below for file uploading. \n",
    "# Otherwise, just ignore it.\n",
    "'''\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "'''\n",
    "wine_dataset = pd.read_csv('./winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "['citric acid', 'fixed acidity', 'pH', 'sulphates', 'volatile acidity', 'chlorides', 'alcohol', 'free sulfur dioxide', 'residual sugar', 'density', 'total sulfur dioxide']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.106845</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.098940</td>\n",
       "      <td>0.567548</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>0.143573</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.215548</td>\n",
       "      <td>0.494126</td>\n",
       "      <td>0.362205</td>\n",
       "      <td>0.209581</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.133556</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.169611</td>\n",
       "      <td>0.508811</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.191617</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.105175</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.190813</td>\n",
       "      <td>0.582232</td>\n",
       "      <td>0.330709</td>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.106845</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.098940</td>\n",
       "      <td>0.567548</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0       0.247788          0.397260         0.00        0.068493   0.106845   \n",
       "1       0.283186          0.520548         0.00        0.116438   0.143573   \n",
       "2       0.283186          0.438356         0.04        0.095890   0.133556   \n",
       "3       0.584071          0.109589         0.56        0.068493   0.105175   \n",
       "4       0.247788          0.397260         0.00        0.068493   0.106845   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0             0.140845              0.098940  0.567548  0.606299   0.137725   \n",
       "1             0.338028              0.215548  0.494126  0.362205   0.209581   \n",
       "2             0.197183              0.169611  0.508811  0.409449   0.191617   \n",
       "3             0.225352              0.190813  0.582232  0.330709   0.149701   \n",
       "4             0.140845              0.098940  0.567548  0.606299   0.137725   \n",
       "\n",
       "    alcohol  quality  \n",
       "0  0.153846        5  \n",
       "1  0.215385        5  \n",
       "2  0.215385        5  \n",
       "3  0.215385        6  \n",
       "4  0.153846        5  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting Labels to a Binary Classification Problem\n",
    "def Convert_Labels(data):\n",
    "    data.loc[:,'quality'] = np.where(data.loc[:,'quality']>=6, 1, 0)\n",
    "    return data\n",
    "\n",
    "#Scales features to constrain them to lie within the default range (0,1)\n",
    "def DataScaler(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data\n",
    "\n",
    "all_columns = list(wine_dataset)\n",
    "target = ['quality']\n",
    "print(all_columns)\n",
    "features = list(set(all_columns)-set(target))\n",
    "print(features)\n",
    "wine_dataset.loc[:,features] = DataScaler(wine_dataset.loc[:,features])\n",
    "\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0          0.247788          0.397260         0.00        0.068493   0.106845   \n",
      "1          0.283186          0.520548         0.00        0.116438   0.143573   \n",
      "2          0.283186          0.438356         0.04        0.095890   0.133556   \n",
      "3          0.584071          0.109589         0.56        0.068493   0.105175   \n",
      "4          0.247788          0.397260         0.00        0.068493   0.106845   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1594       0.141593          0.328767         0.08        0.075342   0.130217   \n",
      "1595       0.115044          0.294521         0.10        0.089041   0.083472   \n",
      "1596       0.150442          0.267123         0.13        0.095890   0.106845   \n",
      "1597       0.115044          0.359589         0.12        0.075342   0.105175   \n",
      "1598       0.123894          0.130137         0.47        0.184932   0.091820   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
      "0                0.140845              0.098940  0.567548  0.606299   \n",
      "1                0.338028              0.215548  0.494126  0.362205   \n",
      "2                0.197183              0.169611  0.508811  0.409449   \n",
      "3                0.225352              0.190813  0.582232  0.330709   \n",
      "4                0.140845              0.098940  0.567548  0.606299   \n",
      "...                   ...                   ...       ...       ...   \n",
      "1594             0.436620              0.134276  0.354626  0.559055   \n",
      "1595             0.535211              0.159011  0.370778  0.614173   \n",
      "1596             0.394366              0.120141  0.416300  0.535433   \n",
      "1597             0.436620              0.134276  0.396476  0.653543   \n",
      "1598             0.239437              0.127208  0.397944  0.511811   \n",
      "\n",
      "      sulphates   alcohol  quality  \n",
      "0      0.137725  0.153846        0  \n",
      "1      0.209581  0.215385        0  \n",
      "2      0.191617  0.215385        0  \n",
      "3      0.149701  0.215385        1  \n",
      "4      0.137725  0.153846        0  \n",
      "...         ...       ...      ...  \n",
      "1594   0.149701  0.323077        0  \n",
      "1595   0.257485  0.430769        1  \n",
      "1596   0.251497  0.400000        1  \n",
      "1597   0.227545  0.276923        0  \n",
      "1598   0.197605  0.400000        1  \n",
      "\n",
      "[1599 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "label_converted_dataset = Convert_Labels(wine_dataset)\n",
    "print(label_converted_dataset)\n",
    "#As you can see, the quality column (our labels) now has either 0 (for quality<6) and 1 (for quality>=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347091932457786"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick Sanity check that our dataset it indeed relatively balanced\n",
    "label_converted_dataset['quality'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wine = label_converted_dataset.loc[:,'quality']\n",
    "X_wine = label_converted_dataset.drop(target,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine_np= np.asarray(X_wine)\n",
    "y_wine_np= np.asarray(y_wine)\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_np, y_wine_np, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you compare your Neural Network's performance (Training and Test Accuracy) with that of Scikit-learn's built-in model MLPClassifier. We, of course, do not expect you to exceed their performance, but your accuracies should be reasonably close to MLPClassifier's.\n",
    "\n",
    "For the Red Wine Dataset, you should be getting over 70% for both your training as well as test set accuracies. If your MLP's training accuracy hovers around 50%, that is a sign that your model is not learning and you need to go back and fix a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=16, hidden_layer_sizes=(12, 8, 2),\n",
       "              random_state=1, solver='sgd')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_quality_classifier = MLPClassifier(solver='sgd',\n",
    "                                            alpha=0.01, \n",
    "                                            learning_rate_init=0.001,\n",
    "                                            batch_size=16,\n",
    "                                            hidden_layer_sizes=(12,8,2),\n",
    "                                            random_state=1,\n",
    "                                            max_iter=200\n",
    "                                            )\n",
    "\n",
    "wine_quality_classifier.fit(X_wine_train, y_wine_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_quality_classifier.score(X_wine_test, y_wine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your MLP on the Red Wine Dataset\n",
    "\n",
    "First, we instantiate and train a standard MLP (that uses the RELU activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wine_NN_1 = NN()\n",
    "num_epochs= 150\n",
    "lambda_reg= 0.01\n",
    "learning_rate= 0.001\n",
    "batch_size= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wine_NN_1.add_layer('Hidden', 11, 16) #Note that the first layer's weight matrix must be 11 x k , as the input feature vector is 11-dimensional.\n",
    "my_wine_NN_1.add_layer('Hidden', 16, 12)\n",
    "my_wine_NN_1.add_layer('Hidden', 12, 8)\n",
    "my_wine_NN_1.add_layer('Output', 8, 2)\n",
    "my_wine_NN_1.add_layer('Loss', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the training data after epoch 1 is 0.46371976647206004\n",
      "the accuracy on the training data after epoch 2 is 0.46371976647206004\n",
      "the accuracy on the training data after epoch 3 is 0.46371976647206004\n",
      "the accuracy on the training data after epoch 4 is 0.5654712260216848\n",
      "the accuracy on the training data after epoch 5 is 0.6422018348623854\n",
      "the accuracy on the training data after epoch 6 is 0.6422018348623854\n",
      "the accuracy on the training data after epoch 7 is 0.6538782318598833\n",
      "the accuracy on the training data after epoch 8 is 0.6622185154295246\n",
      "the accuracy on the training data after epoch 9 is 0.6713928273561302\n",
      "the accuracy on the training data after epoch 10 is 0.6780650542118432\n",
      "the accuracy on the training data after epoch 11 is 0.6780650542118432\n",
      "the accuracy on the training data after epoch 12 is 0.6788990825688074\n",
      "the accuracy on the training data after epoch 13 is 0.6880733944954128\n",
      "the accuracy on the training data after epoch 14 is 0.6980817347789825\n",
      "the accuracy on the training data after epoch 15 is 0.6939115929941618\n",
      "the accuracy on the training data after epoch 16 is 0.6939115929941618\n",
      "the accuracy on the training data after epoch 17 is 0.6939115929941618\n",
      "the accuracy on the training data after epoch 18 is 0.6989157631359466\n",
      "the accuracy on the training data after epoch 19 is 0.7030859049207673\n",
      "the accuracy on the training data after epoch 20 is 0.7089241034195163\n",
      "the accuracy on the training data after epoch 21 is 0.7147623019182652\n",
      "the accuracy on the training data after epoch 22 is 0.7139282735613011\n",
      "the accuracy on the training data after epoch 23 is 0.7197664720600501\n",
      "the accuracy on the training data after epoch 24 is 0.7239366138448707\n",
      "the accuracy on the training data after epoch 25 is 0.7297748123436196\n",
      "the accuracy on the training data after epoch 26 is 0.7364470391993327\n",
      "the accuracy on the training data after epoch 27 is 0.7364470391993327\n",
      "the accuracy on the training data after epoch 28 is 0.7364470391993327\n",
      "the accuracy on the training data after epoch 29 is 0.7356130108423686\n",
      "the accuracy on the training data after epoch 30 is 0.7381150959132611\n",
      "the accuracy on the training data after epoch 31 is 0.7389491242702252\n",
      "the accuracy on the training data after epoch 32 is 0.7381150959132611\n",
      "the accuracy on the training data after epoch 33 is 0.7381150959132611\n",
      "the accuracy on the training data after epoch 34 is 0.7389491242702252\n",
      "the accuracy on the training data after epoch 35 is 0.7389491242702252\n",
      "the accuracy on the training data after epoch 36 is 0.7397831526271893\n",
      "the accuracy on the training data after epoch 37 is 0.7414512093411176\n",
      "the accuracy on the training data after epoch 38 is 0.7397831526271893\n",
      "the accuracy on the training data after epoch 39 is 0.7397831526271893\n",
      "the accuracy on the training data after epoch 40 is 0.7406171809841534\n",
      "the accuracy on the training data after epoch 41 is 0.7397831526271893\n",
      "the accuracy on the training data after epoch 42 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 43 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 44 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 45 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 46 is 0.7447873227689742\n",
      "the accuracy on the training data after epoch 47 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 48 is 0.7422852376980817\n",
      "the accuracy on the training data after epoch 49 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 50 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 51 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 52 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 53 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 54 is 0.7431192660550459\n",
      "the accuracy on the training data after epoch 55 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 56 is 0.7447873227689742\n",
      "the accuracy on the training data after epoch 57 is 0.74395329441201\n",
      "the accuracy on the training data after epoch 58 is 0.7447873227689742\n",
      "the accuracy on the training data after epoch 59 is 0.7456213511259383\n",
      "the accuracy on the training data after epoch 60 is 0.7472894078398665\n",
      "the accuracy on the training data after epoch 61 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 62 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 63 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 64 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 65 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 66 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 67 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 68 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 69 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 70 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 71 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 72 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 73 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 74 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 75 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 76 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 77 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 78 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 79 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 80 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 81 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 82 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 83 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 84 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 85 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 86 is 0.7581317764804003\n",
      "the accuracy on the training data after epoch 87 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 88 is 0.7581317764804003\n",
      "the accuracy on the training data after epoch 89 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 90 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 91 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 92 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 93 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 94 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 95 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 96 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 97 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 98 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 99 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 100 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 101 is 0.7572977481234362\n",
      "the accuracy on the training data after epoch 102 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 103 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 104 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 105 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 106 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 107 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 108 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 109 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 110 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 111 is 0.7589658048373644\n",
      "the accuracy on the training data after epoch 112 is 0.7597998331943286\n",
      "the accuracy on the training data after epoch 113 is 0.7606338615512928\n",
      "the accuracy on the training data after epoch 114 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 115 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 116 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 117 is 0.7606338615512928\n",
      "the accuracy on the training data after epoch 118 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 119 is 0.7614678899082569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the training data after epoch 120 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 121 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 122 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 123 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 124 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 125 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 126 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 127 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 128 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 129 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 130 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 131 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 132 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 133 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 134 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 135 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 136 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 137 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 138 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 139 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 140 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 141 is 0.7614678899082569\n",
      "the accuracy on the training data after epoch 142 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 143 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 144 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 145 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 146 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 147 is 0.7631359466221852\n",
      "the accuracy on the training data after epoch 148 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 149 is 0.762301918265221\n",
      "the accuracy on the training data after epoch 150 is 0.7606338615512928\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_wine_li_1= my_wine_NN_1.train(X_wine_train, y_wine_train, num_epochs, batch_size, learning_rate, lambda_reg)\n",
    "plot_loss(loss_wine_li_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(my_wine_NN_1.compute_accuracy(X_wine_test, y_wine_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wine_NN_2 = NN()\n",
    "num_epochs= 100\n",
    "lambda_reg= 0.01\n",
    "learning_rate= 0.001\n",
    "batch_size= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intital Value of Vondrick Exponent: 1.8178815417034504\n"
     ]
    }
   ],
   "source": [
    "my_wine_NN_2.add_layer('Hidden', 11, 16) #Note that the first layer's weight matrix must be 11 x k , as the input feature vector is 11-dimensional.\n",
    "my_wine_NN_2.add_layer('Hidden', 16, 12)\n",
    "my_wine_NN_2.add_layer('Hidden_Vondrick', 12, 8)\n",
    "my_wine_NN_2.add_layer('Output', 8, 2)\n",
    "my_wine_NN_2.add_layer('Loss', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the training data after epoch 1 is 0.46371976647206004\n",
      "the accuracy on the training data after epoch 2 is 0.53628023352794\n",
      "the accuracy on the training data after epoch 3 is 0.53628023352794\n",
      "the accuracy on the training data after epoch 4 is 0.53628023352794\n",
      "the accuracy on the training data after epoch 5 is 0.53628023352794\n",
      "the accuracy on the training data after epoch 6 is 0.53628023352794\n",
      "the accuracy on the training data after epoch 7 is 0.5379482902418682\n",
      "the accuracy on the training data after epoch 8 is 0.5804837364470392\n",
      "the accuracy on the training data after epoch 9 is 0.6171809841534612\n",
      "the accuracy on the training data after epoch 10 is 0.6422018348623854\n",
      "the accuracy on the training data after epoch 11 is 0.6797331109257715\n",
      "the accuracy on the training data after epoch 12 is 0.707256046705588\n",
      "the accuracy on the training data after epoch 13 is 0.731442869057548\n",
      "the accuracy on the training data after epoch 14 is 0.7347789824854045\n",
      "the accuracy on the training data after epoch 15 is 0.737281067556297\n",
      "the accuracy on the training data after epoch 16 is 0.7364470391993327\n",
      "the accuracy on the training data after epoch 17 is 0.7406171809841534\n",
      "the accuracy on the training data after epoch 18 is 0.7456213511259383\n",
      "the accuracy on the training data after epoch 19 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 20 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 21 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 22 is 0.7472894078398665\n",
      "the accuracy on the training data after epoch 23 is 0.7472894078398665\n",
      "the accuracy on the training data after epoch 24 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 25 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 26 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 27 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 28 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 29 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 30 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 31 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 32 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 33 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 34 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 35 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 36 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 37 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 38 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 39 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 40 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 41 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 42 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 43 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 44 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 45 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 46 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 47 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 48 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 49 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 50 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 51 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 52 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 53 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 54 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 55 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 56 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 57 is 0.7481234361968306\n",
      "the accuracy on the training data after epoch 58 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 59 is 0.7464553794829024\n",
      "the accuracy on the training data after epoch 60 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 61 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 62 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 63 is 0.749791492910759\n",
      "the accuracy on the training data after epoch 64 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 65 is 0.7489574645537949\n",
      "the accuracy on the training data after epoch 66 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 67 is 0.7506255212677231\n",
      "the accuracy on the training data after epoch 68 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 69 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 70 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 71 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 72 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 73 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 74 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 75 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 76 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 77 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 78 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 79 is 0.7514595496246872\n",
      "the accuracy on the training data after epoch 80 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 81 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 82 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 83 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 84 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 85 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 86 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 87 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 88 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 89 is 0.7522935779816514\n",
      "the accuracy on the training data after epoch 90 is 0.7531276063386155\n",
      "the accuracy on the training data after epoch 91 is 0.7539616346955796\n",
      "the accuracy on the training data after epoch 92 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 93 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 94 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 95 is 0.7564637197664721\n",
      "the accuracy on the training data after epoch 96 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 97 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 98 is 0.755629691409508\n",
      "the accuracy on the training data after epoch 99 is 0.7547956630525438\n",
      "the accuracy on the training data after epoch 100 is 0.7564637197664721\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_wine_li_2= my_wine_NN_2.train(X_wine_train, y_wine_train, num_epochs, batch_size, learning_rate, lambda_reg)\n",
    "plot_loss(loss_wine_li_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(my_wine_NN_2.compute_accuracy(X_wine_test, y_wine_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnVlnB2IUs2E"
   },
   "source": [
    "## Section 2: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8oPTvQMUs2F"
   },
   "source": [
    "You now have intuition for how the backpropagation procedure updates every single node or layer in the neural network with the gradient of the loss function with respect to the specific parameters. Luckily, you will not have to repeat this tedious enumeration in the future as autograd packages can help you track and organize the gradient tracking process. Even better, most modern neural network libraries like PyTorch and TensorFlow have their own autograd versions which abstract gradient calculations into a single function call on your loss function, instantly tracking along a computational neural network graph to quickly update gradients. \n",
    "\n",
    "Equally important to the machine learning pipeline is the process of optimization: actually using the calculated **gradient** at the current values and moving to the next values, which are closer to the optimal arguments to our function. A version of **stochastic gradient descent** was already used in section 1 to train the multilayer perceptron.\n",
    "\n",
    "In a perfect world, once we have an analytical formulation for the gradient of a function, we can go back to the classic technique from Calc 2/Calc 3 of setting the gradient to 0 and calculating the values of the variables at the optimal location. Indeed take \n",
    "\n",
    "$$f(x, y) = x^2 + y^2$$\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\\nabla f(x, y) = \\begin{bmatrix}\n",
    "2x \\\\\n",
    "2y \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "yielding the location of the optima at \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElZrLgDqUs2H"
   },
   "source": [
    "However, this technique does not work well with most functions: it is the exception rather than norm to set values to 0 to get and classify the optima. Moreover as the order of the partial derivatives increases, the resulting polynomials become harder and harder to solve (even if a solution is possible) and the problem quickly becomes computationally intractable. Second order methods (Hessian matrices) are required to further classify if these points are in any way useful (saddle points for example would be severely detrimental stopping points for our optimization problem) and are notoriously difficult computationally. \n",
    "\n",
    "We will now proceed to look at some well-known gradient-based iterative algorithms that have successfully been deployed in training deep learning models. In a typical machine learning pipeline, these optimizers will only be useful after the backpropagation stage is complete. At this stage we have that the network parameters $\\theta$ have an associated gradient with respect to the loss $\\frac{\\delta\\mathcal{L}}{\\delta\\theta}$. This step involves using the gradient to nudge $\\theta$ towards an optimum value, i.e, the $\\theta$ that would yield the lowest possible loss.\n",
    "\n",
    "In reality, loss functions that are encountered in neural networks are parametrized by hundreds, thousands and millions of parameters, and hence it is not always easy to visualize or study the exact properties of optimization algorithms on them. Typically such optimization functions are designed for **convex** functions, which are characterized, amidst others by **Jensen's inequality**, meaning that the function always lies below any surface connecting two points on this surface, or precisely for a function $f: X \\mapsto Y$:\n",
    "\n",
    "$$ f(\\theta \\vec{x} + (1 - \\theta)\\vec{y}) \\leq \\theta \\vec{x} + (1 - \\theta)\\vec{y}$$\n",
    "\n",
    "for $x, y \\in X$ and $0 \\leq \\theta \\leq 1$. In $\\mathbb{R}^3$ the **bowl** or **sphere** function is the archetypical convex function.\n",
    "\n",
    "$$f(x, y) = x^2 + y^2$$\n",
    "\n",
    "While convex theory gives convenient bounds on gradient based optimization, it is not enough to stop here as we do not expect the loss function for our machine learning models to be convex. Optimization research then focuses on studying the behavior of algorithms on test functions that accentuate some of the possible problematic optimization scenarios we might run into on a loss manifold in higher dimensions. For example, one potential issue we have studied in class is that in an iterative optimization process, we might get stuck in a local optima. A potential test function that could especially be indicative of if an optimization algorithm handles this issue, is the following function, which we will call the **mult** function:\n",
    "\n",
    "$$f(x, y) = \\sin(\\sqrt(x^2 + y^2)) $$\n",
    "\n",
    "Another issue could be a point which has different signs to its curvature in different directions, but locally has no gradient, aka a saddle point. A \"test\" function to effectively evaluate an algorithm's performance on saddle points could be \n",
    "\n",
    "$$f(x, y) = x^3 + 3x y^2$$\n",
    "\n",
    "or otherwise known as the **monkey saddle**. \n",
    "\n",
    "As a final example, it might be concerning if a point has a very high gradient in one direction but extremely low gradient aka a high condition number is associated with its eigendecomposition of the Hessian (if you didn't understand this last line, that's fine- the Hessian only comes into play while giving a proof of convergence for convex functions for gradient descent and such theory is beyond the scope of this class. An alternate way of thinking why this is an issue is because it will cause a zig-zag convergence to the optima when we can potentially save a lot of iterations by just taking a step along one axis). A function to test convergence performance emperically for this issue could be one shaped like a taco shell. A well known function of this kind is the **Matyas** function.\n",
    "\n",
    "$$f(x, y) = 0.26(x^2 + y^2) -0.48xy$$\n",
    "\n",
    "Bonus point for figuring out who the **Matyas** function is named after, because I looked forever in the hopes of adding a half-clever note on who Matyas was to improve the readability of this homework with a casual fun fact that has nothing to do with machine learning, but I ended up getting lost online and achieving nothing for 30 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP-FjDTzUs2K"
   },
   "source": [
    "**Implement the said functions below as bowl, mult, monkey and matyas and use the plot function below to visualize what they look like. Add a comment on each to explain what there utility might be as test functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1622701983730,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "bEx2NJO7Us2K"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.colors\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd import elementwise_grad as egrad\n",
    "from scipy.optimize import minimize\n",
    "from collections import defaultdict\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def bowl():\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    Implement the bowl function as defined above.\n",
    "\n",
    "    Add comment here explaining why it is a reasonable test function.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def f_bowl(x, y):\n",
    "        \n",
    "        out = # CODE HERE\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def opt_bowl():\n",
    "        \n",
    "        return np.array([0., 0.])\n",
    "    return f_bowl, opt_bowl\n",
    "\n",
    "    \n",
    "    \n",
    "def mult():\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    Implement the mult function as defined above.\n",
    "\n",
    "    Add comment here explaining why it is a reasonable test function.\n",
    "    '''\n",
    "    \n",
    "    def f_mult(x, y):\n",
    "\n",
    "        out = # CODE HERE\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def opt_mult():\n",
    "        \n",
    "        return np.array([0., 0.])\n",
    "    \n",
    "    return f_mult, opt_mult\n",
    "\n",
    "def monkey():\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    Implement the monkey saddle function as defined above.\n",
    "\n",
    "    Add comment here explaining why it is a reasonable test function.\n",
    "    '''\n",
    "\n",
    "    def f_monkey(x, y):\n",
    "\n",
    "        out = # CODE HERE\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def opt_monkey():\n",
    "        \n",
    "        return np.array([0., 0.])\n",
    "    \n",
    "    return f_monkey, opt_monkey\n",
    "\n",
    "    \n",
    "def matyas():\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    Implement the mult function as defined above.\n",
    "\n",
    "    Add comment here explaining why it is a reasonable test function.\n",
    "    '''\n",
    "\n",
    "    def f_matyas(x, y):\n",
    "\n",
    "        out = # CODE HERE\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def opt_matyas():\n",
    "        \n",
    "        return np.array([0., 0.])\n",
    "        \n",
    "    \n",
    "    return f_matyas, opt_matyas\n",
    "\n",
    "\n",
    "def plot_func(f):\n",
    "    \n",
    "    func, opt = f()\n",
    "    \n",
    "    #Set grid parameters\n",
    "    xmin = -4.5\n",
    "    xmax= 4.5\n",
    "    ymin = -4.5\n",
    "    ymax = 4.5\n",
    "    step = 0.2\n",
    "    \n",
    "    x, y = np.meshgrid(np.arange(xmin, xmax + step, step), np.arange(ymin, ymax + step, step))\n",
    "    z = func(x, y)\n",
    "    cp = opt()\n",
    "    optima = cp.reshape(-1, 1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,6), dpi = 100)\n",
    "    ax = fig.add_subplot(1,2,1,projection='3d')\n",
    "    ax.plot_surface(x, y, z,  rstride=5, cstride=5, alpha = 0.5, cmap=plt.cm.plasma)\n",
    "    cset = ax.contourf(x, y, z, 25, zdir='z', offset=-1, alpha=0.6, cmap=plt.cm.coolwarm)\n",
    "    out1 = func(*optima)\n",
    "    \n",
    "    ax.plot(*optima, out1 , 'r*', markersize=10)\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2361,
     "status": "ok",
     "timestamp": 1622701988195,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "Q4k00DBqUs2L",
    "outputId": "95392097-d7bb-4106-df50-441af81ab218",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_func(monkey)\n",
    "plot_func(mult)\n",
    "plot_func(matyas)\n",
    "plot_func(bowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiUUb5ygUs2M"
   },
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FR_L_lYpUs2N"
   },
   "source": [
    "$$ \\theta = \\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta)$$\n",
    "\n",
    "We computed the gradient with respect to each of the parameters and make an update in the opposite direction of the local gradient. $\\alpha$ which is the learning rate, is a hyperparam that controls how quickly gradient descent converges. If it is too low, too many updates may be required, especially as gradients are small, and if it is too large you may overshoot the optima. In the context of batch gradient descent- at each epoch where the above update is ran- batch gradient descent requires that the entire loss function be computed at each stage through a pass through the entire training data so that the calculation of the $\\mathcal{L}(\\theta)$ and its subsequent gradient be accurate. As a result, this method is usually very slow. Additionally, it is only guaranteed to converge to the global minimum for convex functions such as the bowl functions and this is often not the case in machine learning, in which case it is guaranteed to only converge to local minima.\n",
    "\n",
    "Implement a version of gradient descent below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j_Tx2_BAUs2N"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, dx, dy, hparams):\n",
    "    '''\n",
    "    x: value of x before update\n",
    "    y: value of y before update\n",
    "    dx: derivative wrt x\n",
    "    dy: derivative wrt y\n",
    "    hparams must contain alpha\n",
    "\n",
    "    TODO\n",
    "\n",
    "    Implement the update rule and return the new value of x, y after the update\n",
    "    '''\n",
    "    \n",
    "    alpha = hparams['alpha']\n",
    "\n",
    "    #CODE HERE\n",
    "    \n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzjkajLRUs2O"
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_40-q-vwUs2O"
   },
   "source": [
    "$$ \\theta = \\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta; x^i, y^i)$$\n",
    "\n",
    "This method is very similar to full gradient descent, except that instead of calculating the loss function over all training examples, the loss function is only calculated over a single training data point at a time. As such the estimate of $\\mathcal{L}(\\theta)$ and its gradient is not precise, but in return, SGD is much faster, and additionally reduces redundant time for recomputing gradients for similar examples.\n",
    "\n",
    "Gradients received can be highly erratic because they are not calculated over the full dataset, and as a result the optimization path will often zig zag and occasionally spiral out of control. It has been shown that with enough control over the learning rate, sgd and batch gradient descent often achieve the same results, but SGD does it much faster in the context of machine learning. A commonly used variant of gradient descent that sits between full and stochastic gradient descent is mini-batch gradient descent where the gradient and loss function are calculated over a randomly chosen fixed size batch of training examples.\n",
    "\n",
    "Implement a version of stochastic gradient descent below. Since we are not actually using a dataset or even a data distribution to generate our \"loss\" manifold that we are trying to optimize over, you should simulate the effect of approximation using gaussian noise on the gradients. Feel free to use the gauss function imported below for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "609qkBlXUs2P"
   },
   "outputs": [],
   "source": [
    "from random import gauss\n",
    "\n",
    "def stochastic_gradient_descent(x, y, dx, dy, hparams):\n",
    "    '''\n",
    "    x: value of x before update\n",
    "    y: value of y before update\n",
    "    dx: derivative wrt x\n",
    "    dy: derivative wrt y\n",
    "    hparams must contain alpha\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    Implement the update rule and return the new value of x, y after the update\n",
    "    '''\n",
    "    \n",
    "    alpha = hparams['alpha']\n",
    "    \n",
    "    #CODE HERE\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTxhYShxUs2P"
   },
   "source": [
    "## Stochastic Gradient Descent and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iClJpbypUs2P"
   },
   "source": [
    "$$ \\nu_t = \\gamma\\nu_{t-1} + \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta; x^i, y^i) $$\n",
    "$$ \\theta = \\theta - \\nu_t $$\n",
    "\n",
    "Stochastic Gradient Descent suffers very heavily if the loss function changes very quickly in one direction and slowly in another direction (eg. taco shell function) In this case the direction of the gradient does not align with the direction toward the minimum and a zig zag motion is direction with slower gradients. This is the case where the Hessian of the loss function wrt parameters has a high condition number, aka the eigenvalues of the representative matrix have a high ratio between the highest and the lowest eigenvalue. \n",
    "\n",
    "The use of the word momentum is a metaphor in this context, as such a method allows the navigation of shallow local optima or the navigation of ravines using the build-up of gradients from the navigation. The idea is that we maintain what is a 'velocity' term $\\nu$ at each timestep that keeps track of how much gradient has so far been encountered. Hence this value builds up in each direction and even in cases when gradients received in training are poor (eg. around saddle points, or local minima), the algorithm is able to escape such points (much like a ball rolling down a hill). The term for gamma, which is a hyperparameter, can be thought of as friction for the build-up of this velocity, as it decides how much of the previous velocity to count at a certain timestep. Even in the case of a ravine, the zig-zag motion of stochastic gradient descent would reduce as the buildup term in one direction would carry us smoothly through the low-gradient sensitive dimension. The momentum vectors also help cancel/smooth some of the noise that results from approximating gradients using a single data point, aka, the stochastic way.\n",
    "\n",
    "This method has its own flaws: it might settle in extremely deep minima. Such minima are not desirable even as global minima, as on a data manifold they may be \"too good\" to be true; representative of a kind of overfit. An example test function for this issue is the **Easom function**. The good news is that with some tuning of **gamma** more often that never SGD + momentum will settle in shallow, wide minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1621573468866,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "et-Dz4PoUs2Q",
    "outputId": "87561567-ab02-4162-8c5a-9d25bbb34af7"
   },
   "outputs": [],
   "source": [
    "def easom():\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    Implement the easom function as defined above.\n",
    "\n",
    "    Add comment here explaining why it is a reasonable test function.\n",
    "    '''\n",
    "    \n",
    "    def f_easom(x, y):\n",
    "        #CODE HERE\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def opt_easom():\n",
    "        \n",
    "        return np.array([np.pi, np.pi])\n",
    "    \n",
    "    \n",
    "    return f_easom, opt_easom\n",
    "        \n",
    "plot_func(easom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij58vR1kUs2R"
   },
   "source": [
    "Implement your version of stochastic gradient descent with momentum below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wGVIyMb3Us2R"
   },
   "outputs": [],
   "source": [
    "def momentum(x, y, dx, dy, v_x, v_y, hparams):\n",
    "    '''\n",
    "    x: value of x before update\n",
    "    y: value of y before update\n",
    "    dx: derivative wrt x\n",
    "    dy: derivative wrt y\n",
    "    v_x: velocity parameter wrt x\n",
    "    v_y: velocity parameter wrt y\n",
    "    hparams must contain alpha and gamma\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    Implement the update rule and return the new value of x, y, v_x, v_y after \n",
    "    the update. Don't forget to add the gaussian noise for the stochasticity\n",
    "    '''\n",
    "    \n",
    "    alpha = hparams['alpha']\n",
    "    gamma = hparams['gamma']\n",
    "    \n",
    "    #CODE HERE\n",
    "    \n",
    "    return x, y, v_x, v_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ3rFNFLUs2S"
   },
   "source": [
    "## Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXwP47ABUs2S"
   },
   "source": [
    "$$ \\nu_t = \\gamma\\nu_{t-1} + \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta - \\gamma\\nu_{t-1}) $$\n",
    "$$ \\theta = \\theta - \\nu_t $$\n",
    "\n",
    "A problem with momentum is that there is no way to control the slow-down of the optimizer even as we approach a minima- the optimizer isn't smart enough to decide whether or not it needs to continue up a slope once it has reached a (local or global) minima. A version of momentum, called Nesterov momentum helps deal with this problem by first changing parameters in the direction of the accumulated gradient, estimating the destination gradient and then making an update in that direction.\n",
    "\n",
    "You do not have to implement nesterov updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvOpNCWpUs2S"
   },
   "source": [
    "## AdaGrad\n",
    "\n",
    "Adagrad is another approach toward solving some of the same problems that momentum attempts to solve. However, it does more by giving the optimizer the ability to adapt updates for individual parameters depending on their importance. It adapts its learning rate to make higher updates (higher learning rate) for dimensions/features with higher values and lower updates (low learning rate) for dimensions/features with lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5FHIifFUs2T"
   },
   "source": [
    "$$ G_{i, i}^{t+1} = G_{i, i}^{t} + (\\frac{\\delta\\mathcal{L}(\\theta)}{\\delta\\theta_i})^2 $$\n",
    "$$ \\theta_{t,i} = \\theta_{t-1, i} - \\frac{\\alpha}{\\sqrt{G_{i,i} + \\epsilon}}\\nabla_{\\theta}\\mathcal{L}(\\theta_{t, i}) $$\n",
    "\n",
    "Here $G$ starts off as a 0 matrix and builds up squared gradients for each feature/dimension which is later used to scale the value for the parameter updates. $\\epsilon$ is generally a very small number to ensure that division by zero does not occur in case the initialization point provides no gradient. The biggest advantage of adagrad is that it takes away the need to manually tune the learning rate as is required with SGD, and provides feature-tuned learning rates. However, because of the way adagrad adjusts learning rates it sometimes tends to work well only with convex problems, as with non-convex problems in runs into issues with how to overshooting local minima/saddle points as it slows down rapidly due to accumulating squared gradients. It also provides really slow initial updates if the gradients at the initialization point are really high.\n",
    "\n",
    "Implement your variant of Adagrad below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v8hcigpmUs2T"
   },
   "outputs": [],
   "source": [
    "def adagrad(x, y, dx, dy, v_x, v_y, hparams):\n",
    "    '''\n",
    "    x: value of x before update\n",
    "    y: value of y before update\n",
    "    dx: derivative wrt x\n",
    "    dy: derivative wrt y\n",
    "    v_x: velocity parameter wrt x\n",
    "    v_y: velocity parameter wrt y\n",
    "    hparams must contain eps and alpha\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    Implement the update rule and return the new value of x, y, v_x, v_y after \n",
    "    the update. Don't forget to add the gaussian noise for the stochasticity.\n",
    "    '''\n",
    "    \n",
    "    eps = hparams['eps']\n",
    "    alpha = hparams['alpha']\n",
    "    \n",
    "    #CODE HERE\n",
    "    \n",
    "    return x, y, v_x, v_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IyhnvOfUs2T"
   },
   "source": [
    "**RMSProp** and **AdaDelta** are two different algorithms that combat the aggressive learning rate reduction that comes with AdaGrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCRLHvWKUs2U"
   },
   "source": [
    "## Adaptive Momentum Estimation\n",
    "\n",
    "This is the **best of all worlds** update algorithm. The second line (f) accumulated gradients much like momentum, while the third line (s) accumulates gradient squared for adjusting the learning rates like AdaGrad. These have to be adjusted in lines 3 and 4, because they are initialized to 0 at the beginning of optimization and since f is in the numerator of the final update we cannot afford to multiply by a really small number, neither can we afford to divide by a really small number. $\\beta_1$ and $\\beta_2$ must be less than 1 and are typically initialized high values such as 0.9 or 0.99. Yhe unbiasing operations on line 4 and 5 above help bring up the values of f and s early in training. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P76TRRb-Us2U"
   },
   "source": [
    "$$ g_{t, i} = \\nabla_{\\theta}\\mathcal{L}(\\theta_{t, i}) $$\n",
    "$$ f_{t, i} = \\beta_1 f_{t-1, i} + (1-\\beta_1) g_{t, i} $$ \n",
    "\n",
    "$$ s_{t, i} = \\beta_2 s_{t-1, i} + (1-\\beta_2) g_{t, i}^2 $$\n",
    "$$ \\hat{f_{t, i}} = \\frac{f_{t, i}}{1 - \\beta_1^t}$$\n",
    "$$ \\hat{s_{t, i}} = \\frac{s_{t, i}}{1 - \\beta_2^t}$$\n",
    "$$ \\theta_{t+1,i} = \\theta_{t, i} - \\frac{\\alpha\\hat{f_{t, i}}}{\\sqrt{\\hat{s_{t, i}} + \\epsilon}}$$\n",
    "\n",
    "Implement your function for adam below. Follow the comments closely for interpretation of inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kXBx-ZoRUs2U"
   },
   "outputs": [],
   "source": [
    "def adam(x, y, dx, dy, f_x, f_y, s_x, s_y, i, hparams):\n",
    "    '''\n",
    "    x: value of x before update\n",
    "    y: value of y before update\n",
    "    dx: derivative wrt x\n",
    "    dy: derivative wrt y\n",
    "    f_x, f_y: first order gradient accumulators\n",
    "    s_x, s_y: second order gradient accumulators\n",
    "    i: number of iteration\n",
    "    hparams must contain alpha, eps, beta_1 and beta_2\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    Implement the update rule and return the new value of x, y, f_x, f_y, s_x, \n",
    "    s_y, after the update. Don't forget to add the gaussian noise for \n",
    "    the stochasticity.\n",
    "    '''\n",
    "    \n",
    "    eps = hparams['eps']\n",
    "    alpha = hparams['alpha']\n",
    "    beta_1 = hparams['beta_1']\n",
    "    beta_2 = hparams['beta_2']\n",
    "    \n",
    "    #CODE HERE\n",
    "    \n",
    "    return x, y, f_x, f_y, s_x, s_y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwFypBmLUs2V"
   },
   "source": [
    "Study in detail the class Optimizer that has been implemented below, because you will be using it to study the effect of key hyperparameters on the optimization process, and the difference that the bells and whistles on Gradient Descent can make. The **fit** function calls the methods you have implemented above depending on how you initialize the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LKtb-vX8Us2V"
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import math\n",
    "\n",
    "seed(1)\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, x_init, y_init, method, func, hparams):\n",
    "        \n",
    "        '''\n",
    "        x_init: Initialization x point\n",
    "        y_init: Initialization y point\n",
    "        method: adam, adagrad, sgrad, ... check the fit function below\n",
    "        func: function to optimize [mult, easom, monkey, bowl, matyas, ...]\n",
    "        hparams: alpha, gamma, eps, beta, beta_1, beta_2\n",
    "        '''\n",
    "        \n",
    "        f, optima = func()\n",
    "        \n",
    "        self.x = x_init\n",
    "        self.y = y_init\n",
    "        self.hparams = hparams\n",
    "        self.first = True\n",
    "        self.iter = 0\n",
    "        self.x_list = []\n",
    "        self.y_list = []\n",
    "        \n",
    "        cp = optima()\n",
    "        self.cp_x = np.asscalar(np.array([0]))\n",
    "        self.cp_y = np.asscalar(np.array([1]))\n",
    "        \n",
    "        self.method = method\n",
    "        self.f = f\n",
    "        self.f_gradx = grad(f, 0)\n",
    "        self.f_grady = grad(f, 1)\n",
    "        self.count = 0\n",
    "        \n",
    "    def distance(self, x, y):\n",
    "        '''\n",
    "        L2 Norm\n",
    "        '''\n",
    "        \n",
    "        return math.sqrt((x-self.cp_x)**2 + (y-self.cp_y)**2)\n",
    "        \n",
    "    def cgrad(self, x, y):\n",
    "        '''\n",
    "        Uses autograd\n",
    "        '''\n",
    "        \n",
    "        return self.f_gradx(x, y), self.f_grady(x, y)\n",
    "        \n",
    "    def fit(self, epochs):\n",
    "\n",
    "        '''\n",
    "        Epochs: max number of updates to be made\n",
    "        '''\n",
    "\n",
    "        \n",
    "        self.x_list = []\n",
    "        self.y_list = []\n",
    "        self.z_list = []\n",
    "        \n",
    "        if self.method == 'grad':\n",
    "            \n",
    "            '''\n",
    "            USES\n",
    "            alpha: learning rate'\n",
    "            '''\n",
    "            \n",
    "            for i in range(epochs):\n",
    "                \n",
    "                self.x_list.append(self.x)\n",
    "                self.y_list.append(self.y)\n",
    "                self.z_list.append(self.f(self.x, self.y))\n",
    "                \n",
    "                dx, dy = self.cgrad(self.x, self.y)\n",
    "                \n",
    "                self.x, self.y = gradient_descent(self.x, self.y, dx, dy, self.hparams)\n",
    "                \n",
    "                if (self.distance(self.x, self.y) < 1 and self.first):\n",
    "                    self.iter = i\n",
    "                    self.first = False\n",
    "                    \n",
    "                \n",
    "        if self.method == 'sgrad':\n",
    "            \n",
    "            '''\n",
    "            USES\n",
    "            alpha: learning rate\n",
    "            '''\n",
    "            \n",
    "            for i in range(epochs):\n",
    "\n",
    "                self.x_list.append(self.x)\n",
    "                self.y_list.append(self.y)\n",
    "                self.z_list.append(self.f(self.x, self.y))\n",
    "                \n",
    "                dx, dy = self.cgrad(self.x, self.y)\n",
    "                \n",
    "                self.x, self.y = stochastic_gradient_descent(self.x, self.y, dx, dy, self.hparams)\n",
    "                \n",
    "                if (self.distance(self.x, self.y) < 1 and self.first):\n",
    "                    self.iter = i\n",
    "                    self.first = False\n",
    "        \n",
    "                \n",
    "                \n",
    "        if self.method == 'grad_momentum':\n",
    "            \n",
    "            '''\n",
    "            USES\n",
    "            alpha: learning rate\n",
    "            gamma: momentum factor\n",
    "            '''\n",
    "            \n",
    "            v_x, v_y = 0., 0.\n",
    "            for i in range(epochs):\n",
    "                \n",
    "                self.x_list.append(self.x)\n",
    "                self.y_list.append(self.y)\n",
    "                self.z_list.append(self.f(self.x, self.y))\n",
    "                \n",
    "                dx, dy = self.cgrad(self.x, self.y)\n",
    "                \n",
    "                self.x, self.y, v_x, v_y = momentum(self.x, self.y, dx, dy, v_x, v_y, self.hparams)\n",
    "                \n",
    "                if (self.distance(self.x, self.y) < 1 and self.first):\n",
    "                    self.iter = i\n",
    "                    self.first = False\n",
    "                \n",
    "        if self.method == 'adagrad':\n",
    "            \n",
    "            '''\n",
    "            USES\n",
    "            alpha: learning rate\n",
    "            eps: \n",
    "            '''\n",
    "            \n",
    "            v_x, v_y = 0., 0.\n",
    "            for i in range(epochs):\n",
    "                \n",
    "                self.x_list.append(self.x)\n",
    "                self.y_list.append(self.y)\n",
    "                self.z_list.append(self.f(self.x, self.y))\n",
    "                \n",
    "                dx, dy = self.cgrad(self.x, self.y)\n",
    "                \n",
    "                self.x, self.y, v_x, v_y = adagrad(self.x, self.y, dx, dy, v_x, v_y, self.hparams)\n",
    "                \n",
    "                if (self.distance(self.x, self.y) < 1 and self.first):\n",
    "                    self.iter = i\n",
    "                    self.first = False\n",
    "                    \n",
    "                \n",
    "        if self.method == 'adam':\n",
    "            \n",
    "            '''\n",
    "            USES\n",
    "            alpha: learning rate\n",
    "            beta_1:\n",
    "            beta_2:\n",
    "            eps: \n",
    "            '''\n",
    "            \n",
    "            f_x, f_y = 0., 0.\n",
    "            s_x, s_y = 0., 0.\n",
    "            for i in range(epochs):\n",
    "\n",
    "                self.x_list.append(self.x)\n",
    "                self.y_list.append(self.y)\n",
    "                self.z_list.append(self.f(self.x, self.y))\n",
    "                \n",
    "                dx, dy = self.cgrad(self.x, self.y)\n",
    "                \n",
    "                self.x, self.y, f_x, f_y, s_x, s_y = adam(self.x, self.y, dx, dy, f_x, f_y, s_x, s_y, i, self.hparams)\n",
    "                \n",
    "                if (self.distance(self.x, self.y) < 1 and self.first):\n",
    "                    self.iter = i\n",
    "                    self.first = False\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i47wCEjSUs2W"
   },
   "source": [
    "The code at the bottom provides helper functions to help you visualize your results and play around with your optimizer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XT2FC59xUs2X"
   },
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    '''Plotting helper'''\n",
    "    \n",
    "    i = int(i*(epochs/frames))\n",
    "    line1.set_data(optim.x_list[:i+1], optim.y_list[:i+1])\n",
    "    line1.set_3d_properties(optim.z_list[:i+1])\n",
    "    line2.set_data(optim.x_list[:i+1], optim.y_list[:i+1])\n",
    "    line2.set_3d_properties(np.zeros(i+1) -1)\n",
    "    title.set_text('Epoch: {: d}, Error: {:.4f}'.format(i, optim.z_list[i]))\n",
    "    \n",
    "    return line1, line2, title\n",
    "\n",
    "\n",
    "def plot_function(epochs, frames, func, xmin = -4.5, xmax= 4.5, ymin = -4.5, ymax = 4.5, step = 0.2, option = '3d'):\n",
    "\n",
    "    '''\n",
    "    Plot the optimization\n",
    "    epochs: number of optimization improvements\n",
    "    frames: number of displayed frames\n",
    "    func: function to plot. eg. mult, monkey, ....\n",
    "    xmin: Min value of x to be displayed\n",
    "    xmax: Max value of x to be displayed\n",
    "    ymin: Min value of y to be displayed\n",
    "    ymax: Max value of y to be displayed\n",
    "    step: Split between xmin and xmax, ymin and ymax\n",
    "    option: keep this option '3d'\n",
    "\n",
    "    '''    \n",
    "\n",
    "    x, y = np.meshgrid(np.arange(xmin, xmax + step, step), np.arange(ymin, ymax + step, step))\n",
    "    f, opt = func()\n",
    "    z = f(x, y)\n",
    "    cp = opt()\n",
    "    \n",
    "    optima = cp.reshape(-1, 1)\n",
    "    \n",
    "    if (option == '3d'):\n",
    "        \n",
    "        fig = plt.figure(figsize=(12,6), dpi = 100)\n",
    "        ax = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "        ax.plot_surface(x, y, z,  rstride=5, cstride=5, alpha = 0.5, cmap=plt.cm.plasma)\n",
    "        cset = ax.contourf(x, y, z, 25, zdir='z', offset=-1, alpha=0.6, cmap=plt.cm.coolwarm)\n",
    "        out1 = f(*optima)\n",
    "        ax.plot(*optima, out1 , 'r*', markersize=10)\n",
    "\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.set_zlabel('$z$')\n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for i in range(epochs):\n",
    "            line1, = ax.plot(optim.x_list[:i+1], optim.y_list[:i+1], optim.z_list[:i+1], color ='black', marker = '.')\n",
    "\n",
    "            line2, = ax.plot(optim.x_list[:i+1], optim.y_list[:i+1], np.zeros(i+1)-1, color ='red', marker='.')\n",
    "            \n",
    "    \n",
    "    if (option == '2d'):\n",
    "        \n",
    "        fig = plt.figure(dpi = 100)\n",
    "        ax = plt.subplot(111)\n",
    "        \n",
    "        cset = ax.contourf(x, y, z, 25, zdir='z', offset=-1, alpha=0.6, cmap=plt.cm.bwr)\n",
    "        \n",
    "        dz_dx = egrad(f, argnum=0)(x, y)\n",
    "        dz_dy = egrad(f, argnum=1)(x, y)\n",
    "        ax.quiver(x, y, x- dz_dx, y-dz_dy, alpha = 0.5)\n",
    "        ax.plot(*optima, 'r*', markersize = 18)\n",
    "\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "\n",
    "\n",
    "    ax.set_xlim((xmin, xmax))\n",
    "    ax.set_ylim((ymin, ymax))\n",
    "\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 2023,
     "status": "ok",
     "timestamp": 1621573507095,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "hV3jT2vWUs2X",
    "outputId": "83680132-7165-4642-b619-a1f1c51b9132"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Test plot: Here, we plot optimization with adam on bowl function for 200 updates,\n",
    "initialized at (6., 6.).\n",
    "'''\n",
    "\n",
    "frames = 20\n",
    "epochs = 200\n",
    "\n",
    "func = bowl\n",
    "init_x, init_y = 6., 6.\n",
    "\n",
    "hparams = {'alpha': 0.1, 'eps': 1e-7, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "optim = Optimizer(init_x, init_y, 'adam', func, hparams)\n",
    "optim.fit(epochs)\n",
    "\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJmOgDl7Us2Y"
   },
   "source": [
    "# Importance of learning rate\n",
    "\n",
    "Call the plotting function and Optimizer as shown above and make two plots to show how too high or too low of a learning rate could be a problem for stochastic gradient descent on the bowl function. Keep the number of iterations (epochs) constant across the two to demonstrate the effect. You are recommended to use the bowl function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1621573516460,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "fCfanT7pUs2Y",
    "outputId": "0442db71-eb09-4a28-89ad-18cca89fdb23"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if the learning rate is too high with sgrad.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (6., 6.)\n",
    "alpha: 2\n",
    "'''\n",
    "\n",
    "func = bowl\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1621573519147,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "NmbXhCsVUs2Z",
    "outputId": "8cfa8966-1815-438a-8e7b-aa31f1654c8f"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if the learning rate is too low with sgrad.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (6., 6.)\n",
    "alpha: 0.01\n",
    "'''\n",
    "\n",
    "func = bowl\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 6729,
     "status": "ok",
     "timestamp": 1621573529830,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "OBzxAmiHUs2Z",
    "outputId": "1c95af9f-f6ef-4eea-de2f-175a3d427d1b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if the learning rate is just right with sgrad.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (6., 6.)\n",
    "alpha: 0.1\n",
    "'''\n",
    "\n",
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "func = bowl\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvEKYqnQUs2a"
   },
   "source": [
    "# SGD and Matyas: momentum helps\n",
    "\n",
    "Make two plots to show how momentum possibly helps with navigating a taco-shell function like Matyas, where gradient is low in one direction and high in another direction. Keep all parameters except the type of optimizer constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1621573538551,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "ZaltTEyWUs2a",
    "outputId": "aef3478a-5c44-42f3-e0ce-c529b86d6e71"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent is used on a \n",
    "function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (4., -4.)\n",
    "alpha: 0.01\n",
    "'''\n",
    "\n",
    "func = matyas\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1621573552794,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "tmB3hvWmUs2b",
    "outputId": "473d4131-cb69-42cf-df2e-2c8e35e4b474"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent with momentum\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (4., -4.)\n",
    "alpha: 0.01\n",
    "gamma: 0.99\n",
    "'''\n",
    "\n",
    "func = matyas\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnU3WImrUs2b"
   },
   "source": [
    "# Exploring Saddle Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li-Krx0tUs2b"
   },
   "source": [
    "Make three plots to show how SGD, momentum, and adagrad behave on the monkey function. A good optimization algorithm will move past the flat region and continue its trajectory downward. Do not change the learning rate as this has been artificially reduced to make the distinction clear on this problem. Do you notice the observations made in the text above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1621573562652,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "Y6VHeFkFUs2c",
    "outputId": "41640558-a683-49bb-f846-d3512fce40a4"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent is used on a \n",
    "function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (4., 4.)\n",
    "alpha: 0.01\n",
    "'''\n",
    "\n",
    "func = monkey\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1621573567061,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "ZEo5VtJYUs2c",
    "outputId": "d459b1f9-a5e3-4d9a-85ed-6f24bc946f6c"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent with momentum\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (4., 4.)\n",
    "alpha: 0.01\n",
    "gamma: 0.99\n",
    "'''\n",
    "\n",
    "func = monkey\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 10841,
     "status": "ok",
     "timestamp": 1621573579975,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "nF5x_CAfUs2d",
    "outputId": "3e76da37-ef9e-4d35-aa25-c3a26616a631"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 1000\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if adagrad\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (4., 4.)\n",
    "alpha: 0.3\n",
    "eps: 1e-7\n",
    "'''\n",
    "\n",
    "func = monkey\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBKExKCTUs2d"
   },
   "source": [
    "# Escaping Local Minima \n",
    "\n",
    "Make 3 or more plots (with atleast one plot using ADAM) to show how the different functions escape the local minima on the **mult** function. We will initialize at (0.5, 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1621573614810,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "is5cS5K6Us2f",
    "outputId": "9fd50338-8f09-46d3-ac93-c35ab018bc02"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if ADAM\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (.8, .8)\n",
    "alpha: 0.7\n",
    "eps: 1e-7\n",
    "beta_1: 0.99\n",
    "beta_2: 0.9\n",
    "'''\n",
    "\n",
    "\n",
    "func = mult\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1621573600887,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "qj-dPb_RUs2g",
    "outputId": "f972cffe-5661-4322-a1aa-c5b6f4e7f8ee"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if adagrad\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (.8, .8)\n",
    "alpha: 0.7\n",
    "eps: 1e-7\n",
    "'''\n",
    "\n",
    "func = mult\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1621573650486,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "zPQXkby8Us2g",
    "outputId": "fc5d355c-64f8-471f-fdb6-c3cba0a38763"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent with momentum\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (.8, .8)\n",
    "alpha: 0.7\n",
    "gamma: 0.99\n",
    "'''\n",
    "\n",
    "\n",
    "func = mult\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1621573643761,
     "user": {
      "displayName": "Ishaan Preetam Chandratreya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitY9ZPkHr783-fYDTO3yZvHLoeBroKqe3tN9RliA=s64",
      "userId": "13279136665094214597"
     },
     "user_tz": 240
    },
    "id": "1V3x3YosUs2h",
    "outputId": "866a803b-edd4-4756-8f98-a19722502b3f"
   },
   "outputs": [],
   "source": [
    "frames = 20\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "Make a plot to show what happens if stochastic gradient descent with momentum\n",
    "is used on a function like this.\n",
    "\n",
    "Recommended hparams:\n",
    "\n",
    "initialization: (.8, .8)\n",
    "alpha: 0.7\n",
    "'''\n",
    "\n",
    "func = mult\n",
    "#CODE HERE\n",
    "plot_function(epochs, frames, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
